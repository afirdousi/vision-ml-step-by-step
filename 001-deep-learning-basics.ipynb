{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "467f4563-b014-401d-a2eb-1c1e18d88aff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages (1.26.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2fa25c08-3cf5-4883-9e13-dc1762dcd3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7854c11e-951a-45cc-95a6-e3c391079921",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the input variable values (inputs), weights (randomly initialized if this is the first iteration), \n",
    "# and the actual outputs in the provided dataset as the parameters of the feed_forward function\n",
    "\n",
    "# To make this exercise a little more realistic, we will have bias associated with each node. Thus the \n",
    "# weights array will contain not only the weights connecting different nodes but also the bias associated \n",
    "# with nodes in hidden/ output layers.\n",
    "\n",
    "\n",
    "def feed_forward(inputs, outputs, weights):\n",
    "    weight_between_input_to_hidden_layer = weights[0]\n",
    "    bias_between_input_to_hidden_layer = weights[1]\n",
    "    pre_hidden = np.dot(inputs, weight_between_input_to_hidden_layer) + bias_between_input_to_hidden_layer\n",
    "\n",
    "    # Apply the sigmoid activation function on top of the hidden layer values obtained in the previous step – pre_hidden:\n",
    "    hidden = 1/(1 + np.exp(-pre_hidden)) # We will define separate func for activation next\n",
    "\n",
    "    weight_between_hidden_to_output_layer = weights[2]\n",
    "    bias_between_hidden_to_output_layer = weights[3]\n",
    "\n",
    "    # Calculate output by dot producting hidden layer with weights connected to output layer\n",
    "    output_prediction = np.dot(hidden, weight_between_hidden_to_output_layer) + bias_between_hidden_to_output_layer\n",
    "\n",
    "    # Calculate error\n",
    "    mean_squared_error = np.mean(np.square(output_prediction - outputs))\n",
    "                                 \n",
    "    return mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "472f8257-af70-4b41-a625-9226400aaecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definining Activation Functions \n",
    "\n",
    "def tanh(x): \n",
    "    return (np.exp(x)-np.exp(-x))/(np.exp(x)+np.exp(-x))\n",
    "\n",
    "def relu(x):       \n",
    "    return np.where(x>0,x,0)\n",
    "\n",
    "def linear(x):       \n",
    "    return x\n",
    "\n",
    "# Unlike other activations, softmax is performed on top of an array of values. \n",
    "# This is generally done to determine the probability of an input belonging to one \n",
    "# of the m number of possible output classes in a given scenario. Let's say we are \n",
    "# trying to classify an image of a digit into one of the possible 10 classes (numbers from 0 to 9). \n",
    "# In this case, there are 10 output values, where each output value should represent \n",
    "#the probability of an input image belonging to one of the 10 classes.\n",
    "\n",
    "def softmax(x):       \n",
    "    return np.exp(x)/np.sum(np.exp(x))\n",
    "\n",
    "# Here’s how it works:\n",
    "\n",
    "# Input Vector: Suppose you have a vector \n",
    "\n",
    "# Exponential: Apply the exponential function to each element of the vector, which gives \n",
    "# a new vector\n",
    "\n",
    "# Normalization: Divide each exponential by the sum of all exponentials in the vector. \n",
    "# This ensures that the sum of all output values of the softmax function is 1, forming a \n",
    "# valid probability distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9a115af5-f9b0-49fb-b7bc-d2f883873ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definining Loss Functions \n",
    "\n",
    "# The mean squared error is typically used when trying to predict a value that is continuous in nature.\n",
    "def mse(p, y):   \n",
    "    return np.mean(np.square(p - y))\n",
    "\n",
    "# Similar to the mean squared error, the mean absolute error is generally employed on continuous variables. \n",
    "# Further, in general, it is preferable to have a mean absolute error as a loss function when the outputs to predict \n",
    "# have a value less than 1, as the mean squared error would reduce the magnitude of loss considerably (the square of a number between 1 and -1 is an even smaller number) when the expected output is less than 1.\n",
    "# The mean absolute error between an array of predicted output values (p) and an array of actual output values (y) is implemented as follows: \n",
    "def mae(p, y):       \n",
    "    return np.mean(np.abs(p-y))\n",
    "\n",
    "\n",
    "# Binary Cross Entropy\n",
    "# Cross-entropy is a measure of the difference between two different distributions: actual and predicted. \n",
    "# Binary cross-entropy is applied to binary output data\n",
    "# Binary cross-entropy loss has a high value when the predicted value is far away from the actual value and a low value when the predicted and actual values are close.\n",
    "\n",
    "def binary_cross_entropy(p, y):      \n",
    "    return -np.mean(np.sum((y*np.log(p)+(1-y)*np.log(1-p))))\n",
    "\n",
    "\n",
    "# Categorical cross-entropy between an array of predicted values (p) and an array of actual values (y) is implemented as follows: \n",
    "def categorical_cross_entropy(p, y):         \n",
    "    return -np.mean(np.sum(y*np.log(p)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1832bec-b8c8-4036-9dd0-9911477cf876",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we will learn about backpropagation, a technique to adjust weights so that they will result in a loss \n",
    "# that is as small as possible.\n",
    "\n",
    "# In feedforward propagation, we connected the input layer to the hidden layer, which then was connected to the output layer.\n",
    "# In the first iteration, we initialized weights randomly and then calculated the loss resulting from those weight values. \n",
    "# In backpropagation, we take the reverse approach. We start with the loss value obtained in feedforward propagation and \n",
    "# update the weights of the network in such a way that the loss value is minimized as much as possible.\n",
    "\n",
    "# The loss value is reduced as we perform the following steps: \n",
    "# 1. Change each weight within the neural network by a small amount – one at a time. \n",
    "# 2. Measure the change in loss ( ∂L ) when the weight value is changed ( ∂W ). \n",
    "# 3. Update the weight by -k . ∂L/∂W  (where k is a positive value and is a hyperparameter known as the learning rate).\n",
    "\n",
    "# Note that the update made to a particular weight is proportional to the amount of loss that is reduced by changing it \n",
    "# by a small amount. Intuitively, if changing a weight reduces the loss by a large value, then we can update the weight \n",
    "# by a large amount. However, if the loss reduction is small by changing the weight, \n",
    "# then we update it only by a small amount.\n",
    "\n",
    "# If the preceding steps are performed n number of times on the entire dataset (where we have done both the \n",
    "# feedforward propagation and backpropagation), it essentially results in training for n epochs.\n",
    "\n",
    "# Q: Is it practical to update every single weight (millions or billions) in a production setting while training models?\n",
    "\n",
    "# As a typical neural network contains thousands/millions (if not billions) of weights, changing the value of each weight, \n",
    "# and checking whether the loss increased or decreased is not optimal. The core step in the preceding list is the measurement\n",
    "# of \"change of loss\" when the weight is changed. As you might have studied in calculus, measuring this is the same as \n",
    "# computing the gradient of loss concerning the weight. There's more on leveraging partial derivatives from calculus to \n",
    "# calculate the gradient of the loss concerning the weight in the next section, on the chain rule for backpropagation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3d5307f8-3720-4e54-abe2-911cb1744f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets understand one addition concept, Learning Rate before we implement backpropagation\n",
    "\n",
    "# Intuitively, the learning rate helps in building trust in the algorithm. For example, \n",
    "# when deciding on the magnitude of the weight update, we would potentially not change the weight value by a big amount \n",
    "# in one go but update it more slowly. This results in obtaining stability in our model;\n",
    "\n",
    "#This whole process by which we update weights to reduce errors is called gradient descent.\n",
    "\n",
    "# Stochastic gradient descent is how errors are minimized in the preceding scenario. As mentioned earlier, \n",
    "# gradient stands for the difference (which is the difference in loss values when the weight value is updated by \n",
    "# a small amount) and descent means to reduce. Stochastic stands for the selection of random samples based on which \n",
    "# a decision is taken.\n",
    "\n",
    "# Apart from stochastic gradient descent, many other similar optimizers help to minimize loss values, will discuss them later.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "105148a9-d1df-4064-a517-281bcdbefcc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Descent in Code\n",
    "from copy import deepcopy\n",
    "\n",
    "def feed_forward(inputs, outputs, weights):\n",
    "    weight_between_input_to_hidden_layer = weights[0]\n",
    "    bias_between_input_to_hidden_layer = weights[1]\n",
    "    pre_hidden = np.dot(inputs, weight_between_input_to_hidden_layer) + bias_between_input_to_hidden_layer\n",
    "\n",
    "    # Apply the sigmoid activation function on top of the hidden layer values obtained in the previous step – pre_hidden:\n",
    "    hidden = 1/(1 + np.exp(-pre_hidden)) # We will define separate func for activation next\n",
    "\n",
    "    weight_between_hidden_to_output_layer = weights[2]\n",
    "    bias_between_hidden_to_output_layer = weights[3]\n",
    "\n",
    "    # Calculate output by dot producting hidden layer with weights connected to output layer\n",
    "    output_prediction = np.dot(hidden, weight_between_hidden_to_output_layer) + bias_between_hidden_to_output_layer\n",
    "\n",
    "    # Calculate error\n",
    "    mean_squared_error = np.mean(np.square(output_prediction - outputs))\n",
    "                                 \n",
    "    return mean_squared_error\n",
    "    \n",
    "\n",
    "# Increase each weight and bias value by a very small amount (0.0001) and calculate the overall \n",
    "# squared error loss value one at a time for each of the weight and bias updates.\n",
    "\n",
    "def update_weights(inputs, outputs, weights, lr):\n",
    "    # Ensure that you deepcopy the list of weights. As the weights will be manipulated in later steps, \n",
    "    # deepcopy ensures we can work with multiple copies of weights without disturbing actual weights. \n",
    "    # We will create three copies of the original set of weights that were passed as an input to the function – \n",
    "    # 1. original_weights, 2. temp_weights, and 3. updated_weights:\n",
    "\n",
    "    original_weights = deepcopy(weights)\n",
    "    temp_weights = deepcopy(weights)\n",
    "    updated_weights = deepcopy(weights)\n",
    "\n",
    "    # Calculate the loss value (original_loss) with the original set of weights by passing inputs, outputs, and original_weights \n",
    "    # through the feed_forward function:\n",
    "    original_loss = feed_forward(inputs, outputs, original_weights)\n",
    "\n",
    "    # Loop through all the layers of the network:\n",
    "    for i, layer in enumerate(original_weights):\n",
    "        # There are a total of four lists of parameters within our neural network – \n",
    "        # two lists for the weight and bias parameters that connect the input to the \n",
    "        # hidden layer and another two lists for the weight and bias parameters that \n",
    "        # connect the hidden layer to the output layer. Now, we loop through all the \n",
    "        # individual parameters and because each list has a different shape, we leverage\n",
    "        # np.ndenumerate to loop through each parameter within a given list\n",
    "        for index, weight in np.ndenumerate(layer):\n",
    "            temp_weights = deepcopy(weights)\n",
    "            temp_weights[i][index] += 0.0001\n",
    "            _loss_plus = feed_forward(inputs, outputs, temp_weights)\n",
    "            # We calculate the gradient (change in loss value) due to the weight change:\n",
    "            grad = (_loss_plus - original_loss)/(0.0001)\n",
    "            # This ^  process of updating a parameter by a very small amount and then \n",
    "            # calculating the gradient is equivalent to the process of differentiation.\n",
    "\n",
    "            # Finally, we update the parameter present in the corresponding ith layer and\n",
    "            # index, of updated_weights. The updated weight value will be reduced in \n",
    "            # proportion to the value of the gradient. Further, instead of completely reducing\n",
    "            # it by a value equal to the gradient value, we bring in a mechanism to build trust \n",
    "            # slowly by using the learning rate – lr\n",
    "            updated_weights[i][index] -= grad*lr\n",
    "\n",
    "    # Once the parameter values across all layers and indices within layers are updated,\n",
    "    # we return the updated weight values – updated_weights:\n",
    "    return updated_weights, original_loss\n",
    "\n",
    "\n",
    "# In the preceding scenario, we considered all the data points to calculate the loss (mean squared error) value. \n",
    "# However, in practice, when we have thousands (or in some cases, millions) of data points, \n",
    "# the incremental contribution of a greater number of data points while calculating the loss value \n",
    "# would follow the law of diminishing returns, and hence we would be using a batch size that is much smaller \n",
    "# compared to the total number of data points we have. We will apply gradient descent (after feedforward propagation) \n",
    "# using one batch at a time until we exhaust all data points within one epoch of training. \n",
    "\n",
    "# The typical batch size considered in building a model is anywhere between 32 and 1,024.\n",
    "\n",
    "# In this section, we learned about updating weight values based on the change in \n",
    "# loss values when the weight values are changed by a small amount. \n",
    "\n",
    "# In the next section, we will learn about how weights can be updated without computing\n",
    "# gradients one gradient at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1345b5a0-c290-401f-a182-180f3f16281c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing backpropagation using the chain rule\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf35067-c0fd-4983-acc0-a7a27afb1d72",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
