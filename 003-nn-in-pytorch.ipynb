{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db3c0484-0b0b-4de7-a2cd-f891792d184d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages (2.3.1)\n",
      "Requirement already satisfied: filelock in /opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages (from torch) (3.15.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/anasfirdousi/.local/lib/python3.10/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in /opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages (from torch) (1.12.1)\n",
      "Requirement already satisfied: networkx in /opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages (from torch) (2024.6.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: torchvision in /opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages (0.18.1)\n",
      "Requirement already satisfied: numpy in /opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: torch==2.3.1 in /opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages (from torchvision) (2.3.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages (from torchvision) (10.3.0)\n",
      "Requirement already satisfied: filelock in /opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages (from torch==2.3.1->torchvision) (3.15.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/anasfirdousi/.local/lib/python3.10/site-packages (from torch==2.3.1->torchvision) (4.12.2)\n",
      "Requirement already satisfied: sympy in /opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages (from torch==2.3.1->torchvision) (1.12.1)\n",
      "Requirement already satisfied: networkx in /opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages (from torch==2.3.1->torchvision) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages (from torch==2.3.1->torchvision) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages (from torch==2.3.1->torchvision) (2024.6.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages (from jinja2->torch==2.3.1->torchvision) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages (from sympy->torch==2.3.1->torchvision) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch\n",
    "!pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d1d0e03-0438-49c6-b8e9-d33a82460c60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a850015a-d34a-4dda-950e-9b18522c9518",
   "metadata": {},
   "source": [
    "### Components of a Neural Network That We Have Talked About\n",
    "- The number of hidden layers\n",
    "- The number of units in a hidden layer\n",
    "- Activation functions performed at the various layers\n",
    "- The loss function that we try to optimize for\n",
    "- The learning rate associated with the neural network\n",
    "- The batch size of data leveraged to build the neural network\n",
    "- The number of epochs of forward and back-propagation\n",
    "\n",
    "However, for all of these, we built them from scratch using NumPy arrays in Python. In this notebook, we will learn about implementing all of these using PyTorch on a toy dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9f0b2182-4edd-4ee2-806f-cc6fa3b01207",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2.],\n",
      "        [3., 4.],\n",
      "        [5., 6.],\n",
      "        [7., 8.]])\n",
      "tensor([[ 3.],\n",
      "        [ 7.],\n",
      "        [11.],\n",
      "        [15.]])\n"
     ]
    }
   ],
   "source": [
    "# Input and output values\n",
    "\n",
    "x = [[1,2],[3,4],[5,6],[7,8]]\n",
    "y = [[3],[7],[11],[15]] # Output = Sum of values in inpout list\n",
    "\n",
    "# Convert into floats\n",
    "# It is good practice to have tensor objects as floats or long ints, as they will be multiplied by decimal values (weights) anyway.\n",
    "X = torch.tensor(x).float()\n",
    "Y = torch.tensor(y).float()\n",
    "\n",
    "print(X)\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f205213a-d225-4dfc-bcde-d7d55102ee74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register the input (X) and output (Y) data points to the device â€“ cuda if you have a GPU and cpu if you don't have a GPU:\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "X = X.to(device)\n",
    "Y = Y.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3442f892-9fee-48c3-831c-009521ec8e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Neural Network Architecture\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "class MyNeuralNet(nn.Module): # inheriting from nn.Module\n",
    "    def __init__(self):\n",
    "        super().__init__()  # We should call super().__init__() to ensure that the class inherits nn.Module:\n",
    "\n",
    "        # Define layers in NN\n",
    "        self.input_to_hidden_layer = nn.Linear(2,8)\n",
    "        self.hidden_layer_activation = nn.ReLU()\n",
    "        self.hidden_to_output_layer = nn.Linear(8,1)\n",
    "\n",
    "        # For now, the choice of number of layers and activation is arbitrary.\n",
    "        # We'll learn about the impact of the number of units in layers and layer activations in more detail later\n",
    "\n",
    "    # Let's connect the components together while defining the forward propagation of the network\n",
    "    # It is mandatory to use forward as the function name since PyTorch has reserved this function as the method for \n",
    "    # performing forward propagation. Using any other name in its place will raise an error.\n",
    "    def forward(self, x):\n",
    "        x = self.input_to_hidden_layer(x)\n",
    "        x = self.hidden_layer_activation(x)\n",
    "        x = self.hidden_to_output_layer(x)\n",
    "        return x    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0149484e-30df-48a8-892e-ab0a70bf2a7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.4101,  0.5737],\n",
      "        [ 0.3459, -0.0734],\n",
      "        [ 0.5421,  0.1373],\n",
      "        [ 0.2868, -0.0571],\n",
      "        [ 0.1646, -0.6410],\n",
      "        [ 0.5123,  0.1445],\n",
      "        [ 0.6824,  0.3221],\n",
      "        [ 0.5782,  0.7057]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.5125, -0.6074, -0.0498, -0.0398,  0.0804,  0.0949, -0.6229, -0.2854],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# Accessing weights and bias\n",
    "\n",
    "testnn = MyNeuralNet().to(device)\n",
    "print(testnn.input_to_hidden_layer.weight)\n",
    "print(testnn.input_to_hidden_layer.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0b4aeabf-9661-418f-a8ee-ce3c09a26354",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.4101,  0.5737],\n",
      "        [ 0.3459, -0.0734],\n",
      "        [ 0.5421,  0.1373],\n",
      "        [ 0.2868, -0.0571],\n",
      "        [ 0.1646, -0.6410],\n",
      "        [ 0.5123,  0.1445],\n",
      "        [ 0.6824,  0.3221],\n",
      "        [ 0.5782,  0.7057]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.5125, -0.6074, -0.0498, -0.0398,  0.0804,  0.0949, -0.6229, -0.2854],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.1254,  0.2876, -0.1096, -0.0509,  0.1335, -0.0922, -0.0472, -0.3466]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.2388], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# Or get all params at once\n",
    "\n",
    "testnn.parameters() # this returns a generator that you can iterate over\n",
    "\n",
    "for par in testnn.parameters():\n",
    "    print(par)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4effd6d3-bef6-4549-90bd-7b706d7f7d98",
   "metadata": {},
   "source": [
    "- The model has registered these tensors as special objects that are necessary for keeping track of forward and backward propagation.\n",
    "- When defining any nn layers in the __init__ method, it will automatically create corresponding tensors and simultaneously register them.\n",
    "\n",
    "### Manually Register Parameters\n",
    "- You can also manually register these parameters using the nn.Parameter(<tensor>) function. Hence, the following code is equivalent to the neural network class that we defined previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "362c4d66-c50a-4498-85e5-b178f9659bab",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<tokenize>, line 8)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m<tokenize>:8\u001b[0;36m\u001b[0m\n\u001b[0;31m    def forward(self, x):\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "# This model is equivalent to the previous implementation but uses the underlaying Parameter and torch.rand functions\n",
    "class MyNeuralNet(nn.Module):\n",
    "     def __init__(self):\n",
    "        super().__init__()\n",
    "        self.input_to_hidden_layer = nn.Parameter(torch.rand(2,8))\n",
    "        self.hidden_layer_activation = nn.ReLU()\n",
    "        self.hidden_to_output_layer = nn.Parameter(torch.rand(8,1))\n",
    " def forward(self, x):\n",
    "        x = x @ self.input_to_hidden_layer\n",
    "        x = self.hidden_layer_activation(x)\n",
    "        x = x @ self.hidden_to_output_layer\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb799d7-7bfc-475b-86f3-0dbf2379532b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
